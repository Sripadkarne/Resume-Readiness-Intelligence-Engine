{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9158cea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/itw_intel/lib/python3.14/site-packages/langchain_core/_api/deprecation.py:26: UserWarning: Core Pydantic V1 functionality isn't compatible with Python 3.14 or greater.\n",
      "  from pydantic.v1.fields import FieldInfo as FieldInfoV1\n",
      "/opt/anaconda3/envs/itw_intel/lib/python3.14/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Requires pdfplumber (pip install pdfplumber)\n",
    "# Requires langchain-groq (pip install langchain-groq)\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import textwrap\n",
    "from typing import Optional\n",
    "from xml.etree import ElementTree as ET\n",
    "\n",
    "import pdfplumber\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "DEFAULT_MODEL = \"llama-3.1-8b-instant\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c67729cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_text_from_pdf(pdf_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Extracts raw text from the PDF and lightly normalizes whitespace.\n",
    "    \"\"\"\n",
    "    text_chunks = []\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            page_text = page.extract_text() or \"\"\n",
    "            text_chunks.append(page_text)\n",
    "    combined = \"\\n\\n\".join(text_chunks)\n",
    "    normalized_lines = []\n",
    "    for raw_line in combined.splitlines():\n",
    "        squashed = \" \".join(raw_line.split())\n",
    "        normalized_lines.append(squashed)\n",
    "    normalized = \"\\n\".join(line for line in normalized_lines if line)\n",
    "    return normalized.strip()\n",
    "\n",
    "\n",
    "def build_llm_prompt(resume_text: str) -> str:\n",
    "    \"\"\"\n",
    "    Creates the instruction prompt for the LLM, requesting structured XML output.\n",
    "    \"\"\"\n",
    "    instruction = textwrap.dedent(\n",
    "        \"\"\"\n",
    "        You are a résumé parsing assistant. Given the raw résumé text below, extract:\n",
    "        1. Skills section (grouped by category when possible). Only include technical skills/tools/frameworks; drop human language proficiency entries.\n",
    "        2. Work experience entries with position, company, and bullet descriptions (omit any dates or locations entirely).\n",
    "        3. Project entries with name, optional context, and descriptions.\n",
    "        4. Education entries (degree, institution) plus any explicit coursework. Omit dates and location\n",
    "        5. Any remaining noteworthy lines (place under <other>), excluding personal identifiers.\n",
    "\n",
    "        Output well-formed XML that matches this structure exactly:\n",
    "\n",
    "        <resume>\n",
    "          <skills>\n",
    "            <category name=\"Category Name\">\n",
    "              <skill>Example</skill>\n",
    "            </category>\n",
    "          </skills>\n",
    "          <experience>\n",
    "            <job>\n",
    "              <position>...</position>\n",
    "              <company>...</company>\n",
    "              <description>\n",
    "                <bullet>...</bullet>\n",
    "              </description>\n",
    "            </job>\n",
    "          </experience>\n",
    "          <projects>\n",
    "            <project>\n",
    "              <name>...</name>\n",
    "              <context>...</context>\n",
    "              <description>\n",
    "                <bullet>...</bullet>\n",
    "              </description>\n",
    "            </project>\n",
    "          </projects>\n",
    "          <education>\n",
    "            <entry>\n",
    "              <degree>...</degree>\n",
    "              <institution>...</institution>\n",
    "              <courses>\n",
    "                <course>...</course>\n",
    "              </courses>\n",
    "            </entry>\n",
    "          </education>\n",
    "          <other>\n",
    "            <line>...</line>\n",
    "          </other>\n",
    "        </resume>\n",
    "\n",
    "        Always include every top-level section (even if empty) and escape XML special characters. Do not introduce <dates> or <location> elements anywhere, avoid language-only skill categories, and never output names, phone numbers, emails, or addresses.\n",
    "        \"\"\"\n",
    "    ).strip()\n",
    "    resume_block = f\"\\n\\nRésumé text:\\n{resume_text}\"\n",
    "    return instruction + resume_block\n",
    "\n",
    "\n",
    "def call_llm(llm: ChatGroq, prompt: str) -> str:\n",
    "    \"\"\"\n",
    "    Calls the Groq-hosted LLM using the same structure as backend/Agents/skillextractor.py.\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You convert resumes into structured XML that matches the requested schema exactly.\",\n",
    "        ),\n",
    "        (\"user\", prompt),\n",
    "    ]\n",
    "    response = llm.invoke(messages)\n",
    "    return response.content.strip()\n",
    "\n",
    "\n",
    "def _indent(elem: ET.Element, level: int = 0) -> None:\n",
    "    \"\"\"\n",
    "    Pretty-print helper for ElementTree output.\n",
    "    \"\"\"\n",
    "    indent_unit = \"  \"\n",
    "    indent_text = \"\\n\" + level * indent_unit\n",
    "    if len(elem):\n",
    "        if not elem.text or not elem.text.strip():\n",
    "            elem.text = indent_text + indent_unit\n",
    "        for child in elem:\n",
    "            _indent(child, level + 1)\n",
    "            if not child.tail or not child.tail.strip():\n",
    "                child.tail = indent_text + indent_unit\n",
    "        if not elem.tail or not elem.tail.strip():\n",
    "            elem.tail = indent_text\n",
    "    else:\n",
    "        if level and (not elem.tail or not elem.tail.strip()):\n",
    "            elem.tail = indent_text\n",
    "\n",
    "\n",
    "def summarize_xml(root: ET.Element) -> str:\n",
    "    \"\"\"\n",
    "    Derives a simple textual summary from the generated XML.\n",
    "    \"\"\"\n",
    "    def count(path: str) -> int:\n",
    "        return len(root.findall(path))\n",
    "\n",
    "    categories = count(\"./skills/category\")\n",
    "    experiences = count(\"./experience/job\")\n",
    "    projects = count(\"./projects/project\")\n",
    "    education_entries = count(\"./education/entry\")\n",
    "    return (\n",
    "        f\"skill categories: {categories}, experiences: {experiences}, \"\n",
    "        f\"projects: {projects}, education entries: {education_entries}\"\n",
    "    )\n",
    "\n",
    "\n",
    "def extract_xml_fragment(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Strip markdown/code fences or prose surrounding the XML payload.\n",
    "    \"\"\"\n",
    "    cleaned = text.strip()\n",
    "    if \"```\" in cleaned:\n",
    "        fence_start = cleaned.find(\"```\")\n",
    "        after_fence = cleaned[fence_start + 3 :]\n",
    "        if after_fence.lower().startswith(\"xml\"):\n",
    "            after_fence = after_fence[3:]\n",
    "        fence_end = after_fence.find(\"```\")\n",
    "        if fence_end != -1:\n",
    "            cleaned = after_fence[:fence_end].strip()\n",
    "        else:\n",
    "            cleaned = after_fence.strip()\n",
    "    if \"<resume\" in cleaned and \"</resume>\" in cleaned:\n",
    "        start = cleaned.find(\"<resume\")\n",
    "        end = cleaned.rfind(\"</resume>\") + len(\"</resume>\")\n",
    "        cleaned = cleaned[start:end]\n",
    "    return cleaned.strip()\n",
    "\n",
    "\n",
    "def main(argv: Optional[list[str]] = None) -> None:\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=\"Convert a résumé PDF into structured XML using an LLM.\",\n",
    "    )\n",
    "    parser.add_argument(\"input_pdf_path\", help=\"Path to the PDF résumé.\")\n",
    "    parser.add_argument(\n",
    "        \"output_xml_path\",\n",
    "        nargs=\"?\",\n",
    "        default=\"resume_parsed.xml\",\n",
    "        help=\"Where to write the XML output (default: resume_parsed.xml).\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--model\",\n",
    "        default=DEFAULT_MODEL,\n",
    "        help=f\"Model name to request (default: {DEFAULT_MODEL}).\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--max-tokens\",\n",
    "        type=int,\n",
    "        default=2000,\n",
    "        help=\"Maximum number of tokens to request from the LLM (default: 2000).\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--api-key\",\n",
    "        help=\"Optional GROQ_API_KEY override; otherwise the environment variable must be set.\",\n",
    "    )\n",
    "    args = parser.parse_args(argv)\n",
    "\n",
    "    if args.api_key:\n",
    "        os.environ[\"GROQ_API_KEY\"] = args.api_key\n",
    "    if \"GROQ_API_KEY\" not in os.environ or not os.environ[\"GROQ_API_KEY\"]:\n",
    "        parser.error(\"Set GROQ_API_KEY in the environment or pass --api-key.\")\n",
    "\n",
    "    llm = ChatGroq(\n",
    "        model=args.model,\n",
    "        temperature=0,\n",
    "        max_tokens=args.max_tokens,\n",
    "        timeout=None,\n",
    "        max_retries=2,\n",
    "    )\n",
    "\n",
    "    resume_text = extract_text_from_pdf(args.input_pdf_path)\n",
    "    prompt = build_llm_prompt(resume_text)\n",
    "    llm_output = call_llm(llm, prompt)\n",
    "    xml_payload = extract_xml_fragment(llm_output)\n",
    "\n",
    "    try:\n",
    "        root = ET.fromstring(xml_payload)\n",
    "    except ET.ParseError as exc:\n",
    "        raise RuntimeError(f\"LLM output is not valid XML:\\n{llm_output}\") from exc\n",
    "\n",
    "    _indent(root)\n",
    "    tree = ET.ElementTree(root)\n",
    "    tree.write(args.output_xml_path, encoding=\"utf-8\", xml_declaration=True)\n",
    "    print(f\"Wrote {args.output_xml_path} ({summarize_xml(root)})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e1282d8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sripad Karne\\n858-275-3303 | sk5695@columbia.edu | linkedin.com/in/sripad-karne | US Citizen\\nEducation\\nColumbia University New York City, NY\\nMaster of Science (M.S.) in Data Science Expected Dec. 2026\\nUniversity of California, San Diego San Diego, CA\\nBachelor of Science (B.S) in Cognitive Science w/ specialization in Machine Learning Sep. 2021 – June 2025\\nExperience\\nData Science Intern May 2025 – Sep. 2025\\nUC San Diego Health San Diego, CA\\n• Supporting clinical research at UC San Diego Health by processing and analyzing large-scale EHR datasets (>10\\nmillion patients) on Databricks and GCP using Spark SQL, helping integrate data across hospitals using\\nstandardized clinical terminologies (ICD-10, CPT, etc.)\\n• Conducting advanced exploratory data analysis in Python to identify key clinical and demographic risk factors for\\nchronic conditions such as lung cancer and CKD, informing early detection and personalized risk stratification\\n• Developing and optimizing machine learning models, such as XGBoost and Random Forest, to model disease\\nprogression and generate individualized risk trajectories, contributing to an upcoming manuscript on ML-driven\\nCKD prediction\\n• Designing visualizations and presenting data-driven insights to key partners, including LifeGuard Health Networks\\nand Stand Up To Cancer, to support clinical research and decision-making initiatives\\nInstructional Assistant for Advanced Machine Learning Methods Mar. 2025 – June 2025\\nUniversity of California, San Diego San Diego, CA\\n• Conducted weekly office hours and sessions to deepen student understanding of ML methods, from regression and\\nclassification to Generative Modeling, Transformers, and LLM models\\n• Provided individualized guidance on complex topics, offering debugging support, and clarifying assignments\\nComputer Vision Intern June 2024 – Mar. 2025\\nMachine Learning, Perception & Cognition Lab San Diego, CA\\n• Designed a novel controllable image generation architecture, leveraging cutting-edge Diffusion Transformers and\\nstate-of-the-art deep learning models\\n• Integrated LLMs into data pipelines for processing multi-terabyte video datasets, using them to extract and refine\\nopen-vocabulary object prompts and dynamically guide bounding box detection at scale\\n• Conducted in-depth model testing and evaluation, applying various performance metrics and validation techniques\\nto ensure the robustness, accuracy, and efficiency of the model\\nData Analyst Intern Jan. 2023 – Apr. 2024\\nNeural Engineering and Translation Labs San Diego, CA\\n• Developed and implemented machine learning models to analyze behavioral and questionnaire data, generating\\npersonalized recovery plans for patients displaying signs of depression and other mental health conditions\\n• Contributed to the design and implementation of efficient data pipelines to streamline data flow and enhance data\\naccessibility for the research teams, as well as doing rigorous data exploration, cleansing, and preprocessing to\\nensure data integrity\\nPublications\\nHwang, H., Yoon, S., Karne, S., Boussina, A., & Sitapati, A. (in progress). Machine learning–derived risk\\ntrajectory of chronic kidney disease and clinical implications after disease onset.\\nTechnical Skills\\nLanguages: Python, SQL, R, MATLAB\\nFrameworks & Libraries: PyTorch, TensorFlow, AWS, GCP, Snowflake, Databricks, NLP, LLM, Docker, Apache\\nSpark, Pandas, NumPy, Scikit-learn, XGboost, RandomForest, PySpark, Excel, Tableau, Git, Linux, LangChain\\nRelevant Coursework: Advanced Machine Learning Methods, Modeling and Data Analysis, AI Algorithms, AI\\nEngineering, Algorithms for Data Science, Neural Networks and Deep Learning, Exploratory Data Analysis and\\nVisualization, Business Analytics'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_path = \"/Users/alexandresepulvedadedietrich/Documents/Columbia/Fall_Term/AI_eng_apps/Resume-Readiness-Intelligence-Engine/backend/Agents/resume_parsing/resume_pdf/STR_ML_CVIntern_Resume.pdf\"\n",
    "\n",
    "extract_text_from_pdf(pdf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29999b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        main()\n",
    "    except Exception as exc:\n",
    "        print(f\"Error: {exc}\", file=sys.stderr)\n",
    "        sys.exit(1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "itw_intel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
